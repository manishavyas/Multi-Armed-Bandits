# Multi-Armed-Bandits

With the use of A/B Testing, two major challenges are faced by the online subscription industry today, i.e., to maximize conversions of trial users while testing product variants and to address ever changing user preferences. 
This project involves performance evaluation of Multi-armed Bandit Algorithms, as alternatives to conventional A/B testing, that balances exploration and exploitation during the learning process to quickly identify the overall winning feature variant. 
Contextual Bandit Algorithms are also explored to predict what works best for a given user based on their attributes. This approach helps optimize the conversion rate and support personalization at the user level. 
